# **BoostFormer**

![Main](https://user-images.githubusercontent.com/103131249/217564941-4a04a02f-57fe-4a85-9cdc-6ff287687cba.png)

</br>

## ğŸ“° **Contributors**

**CV-16ì¡° ğŸ’¡ ë¹„ì „ê¸¸ì¡ì´ ğŸ’¡**</br>NAVER Connect Foundation boostcamp AI Tech 4th

|ë¯¼ê¸°|ë°•ë¯¼ì§€|ìœ ì˜ì¤€|ì¥ì§€í›ˆ|ìµœë™í˜|
|:----:|:----:|:----:|:---:|:---:|
|[<img alt="revanZX" src="https://user-images.githubusercontent.com/70796031/217557235-d89557b2-a178-4650-8c21-20cf2b7f80b8.png" width="100%">](https://github.com/revanZX)|[<img alt="arislid" src="https://user-images.githubusercontent.com/70796031/217557228-c0e572a4-40c6-44ae-8d53-f28c32fe4e6d.png" width="100%">](https://github.com/arislid)|[<img alt="youngjun04" src="https://user-images.githubusercontent.com/70796031/217557229-e42381d6-4c27-482c-801e-49872bcedd30.png" width="100%">](https://github.com/youngjun04)|[<img alt="FIN443" src="https://user-images.githubusercontent.com/70796031/217557222-1491ed63-3587-42a1-9bf2-591c91e52e36.png" width="100%">](https://github.com/FIN443)|[<img alt="choipp" src="https://user-images.githubusercontent.com/70796031/217557214-03420adf-5950-4710-b857-a289961989c6.png" width="100%">](https://github.com/choipp)|
<!-- |Remove SelfOutput</br>Separable Attention</br>Inverted Residual Mobile Block</br>Project GitHub Management | Cross-Covariance Attention</br>Inverted Residual Mobile Block</br>ì‹¤í—˜ ê¸°ë¡ Â· ë§¤ë‰´ì–¼ ì‘ì„± ë° ê´€ë¦¬</br>Hyperparameter Tuning (Batch Size)</br>Deploy models on Jetson Nano | Weighted Sum</br>Sequence Reduction Pooling</br>ê°œì„ ëœ êµ¬ì¡° ë° ê¸°ë²• ë³‘í•©</br>Project Documentation</br>Ablation Study | Deformable Attention</br>MixCFN</br>Local Connection</br>Inference Results ì¶”ì¶œ Â· ë¶„ì„</br>Profiling Tool ì½”ë“œ ê°œì„  | PM</br>Pool Former</br>Pooling Patch Embedding</br>Learnable Resizer</br>ë ˆì´ì–´ë³„ Params Â· FLOPs ë¶„ì„| -->

<!-- ![profile](https://user-images.githubusercontent.com/70796031/217537194-9a9af88d-bd74-4421-bb9e-43ccd302f916.png) -->
</br>

## ğŸ“° **Links**

- [ë¹„ì „ ê¸¸ì¡ì´ Notion ğŸ“](https://vision-pathfinder.notion.site/NOTA-AI-Semantic-Segmentation-1669f7d1c9bc4f39b83ac05dd547da9b)
- [ë¹„ì „ ê¸¸ì¡ì´ ë°œí‘œìë£Œ & WrapUpReport](./appendix/)

## ğŸ“° **Objective**
![image](https://user-images.githubusercontent.com/103131249/217568159-2cbb62b9-ff6c-4795-8d7c-7f637f90e95e.png)
- **SegFormer** : ì„ë² ë””ë“œ ë° ëª¨ë°”ì¼ ê¸°ê¸°ë¥¼ ìœ„í•œ Transformer ê¸°ë°˜ Semantic Segmentation ëª¨ë¸ ê²½ëŸ‰í™”
- **Model driven approach** : í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ë“± ê³ ë„í™”ëœ í•™ìŠµ ê¸°ë²• ë°°ì œ Â· ìˆœìˆ˜ ëª¨ë¸ë§ì„ í†µí•œ ì„±ëŠ¥ í–¥ìƒ
- Pruning ë° quantization ë“± **compression ë°©ë²• ë°°ì œ** : ëª¨ë¸ ë¸”ë¡ Â· ë ˆì´ì–´ ì¬ì„¤ê³„ ë“± ê²½ëŸ‰í™” êµ¬ì¡°ë³€ê²½ ì§„í–‰


## ğŸ“° **Dataset**

||Tiny-ImageNet|ADE20K|
|:----:|:---:|:---:|
||<img src="https://user-images.githubusercontent.com/113173095/217479439-5492f4b3-a115-43b7-9952-b1c1a5c850b9.png" width="300" height="300">|<img src="https://user-images.githubusercontent.com/113173095/217480114-cc9dbb9b-deaa-4620-bb78-920da8d06b84.png" width="300" height="150">|
|Purpose|Pre-training|Fine-tuning|
|Num_classes|200|150|
|Training set|100,000 images|20,210 images|
|Validation set|10,000 images|2,000 images|

```
|-- ADEChallengeData2016
|   |-- image
|   |   |-- train
|   |   `-- val
|   `-- mask
|       |-- train
|       `-- val
`-- tiny-imagenet-200
    |-- train
    |-- val
```
<!-- ### Tiny-ImageNet (Pre-training)
<img src="https://user-images.githubusercontent.com/113173095/217479439-5492f4b3-a115-43b7-9952-b1c1a5c850b9.png" width="200" height="200">

- Num_classes : 200
- Training set : 100,000 images
- Validation set : 10,000 images
- Image Size : 64 x 64

### ADE20K (Fine-tuning)
<img src="https://user-images.githubusercontent.com/113173095/217480114-cc9dbb9b-deaa-4620-bb78-920da8d06b84.png" width="200" height="100">

- Num_classes : 150
- Training set : 20,210 images
- Validation set : 2,000 images -->

## ğŸ“° **Base Model**
![Segformer](https://user-images.githubusercontent.com/103131249/217569843-478c191b-e431-4903-9e36-6259d2bd990a.png)
<!-- <img src="https://user-images.githubusercontent.com/113173095/217485570-003dd0a5-6f0d-4195-8e73-c9ca64f57e02.png" width="400" height="300"> -->
|Encoder|Decoder|
|:---:|:---:|
|Overlap Patch Embedding|MLP Layer (upsampling)|
|SegFormer Block|Concat|
|Efficient Self-Attention|Linear-Fuse|
|Mix-FFN|Classifier|
---

## ğŸ“° **BoostFormer(Ours)**
![boostformer](https://user-images.githubusercontent.com/70796031/217690146-e7e226df-4a8b-4d99-a1b6-d5d7f3517d9f.png)

|Encoder|Decoder|
|:---:|:---:|
|**Poolin Patch Embedding**|MLP Layer (upsampling)|
|**PoolFormer Block**|**Weighed Sum**|
|**SegFormerV2 Block**|Classifier|
|**Custom Efficient Self-Attention**|-|
|**Mix-CFN**|-|

---

## ğŸ“° **Strategy**

![image](https://user-images.githubusercontent.com/103131249/217567344-f2be7b76-c18c-4157-8770-ba80142540b9.png)

- Segformer-B2ì™€ custom model ì„±ëŠ¥ ë¹„êµ ë° Paramsì™€ Flops ì¸¡ì • (util/get_flops_params.py)

---

## ğŸ“° **Method**

### 1. Patch Embedding

<img src="https://user-images.githubusercontent.com/25689849/217509298-cabb401f-e736-4c44-8719-15830b487b97.svg">

- NxN Convë¥¼ Pooling + 1x1 Convë¡œ ëŒ€ì²´

    <img src="https://user-images.githubusercontent.com/103131249/217582684-3601ac28-0886-481a-aee0-bd52b26fa7f9.png" width="150">
    

### 2. Transformer Block
<img src="https://user-images.githubusercontent.com/25689849/217509038-98c57ecc-ff32-4f74-8f36-caab02bc1fcb.svg">

- Token Mixer : MHSA ëŒ€ì‹  Poolingìœ¼ë¡œ feature ì¶”ì¶œ
    - $\hat {F_0}=\mathrm {LayerScale}(\mathrm {Pooling}(F_{in}))+F_{in}$
    - $\hat {F_1}=\mathrm {LayerScale}(\mathrm {MixCFN}(\hat {F_0}))+\hat {F_0}$
- ê¸°ì¡´ Self Output ëª¨ë“ˆ ì‚­ì œ
    - $\hat {F_0}=\mathrm {CSA}(F_{in})+F_{in}$
    - $\hat {F_1}=\mathrm {MixCFN}(\hat {F_0})+\hat {F_0}$

### 3. Attention Layer

<img src="https://user-images.githubusercontent.com/25689849/217508647-14b486db-85ed-4684-a652-d3bc30c6e334.svg">

- Poolingìœ¼ë¡œ K, V ì°¨ì› ì¶•ì†Œ
    - $K, V=\mathrm {Pooling}(F_C)$
    
- 1x1 Convolution ì‚­ì œ
    - $\mathrm {Attention}(Q,K,V)=\mathrm {Softmax}({{QK^T}\over {\sqrt {d_{head}}}}V)$

### 4. FFN

<img src="https://user-images.githubusercontent.com/25689849/217507844-f29f7b99-d143-4166-b60a-4a29f643e38e.svg">

- ê¸°ì¡´ì˜ Linear(dense) embedding ì—°ì‚°ì„ 1x1 Convë¡œ ë³€ê²½
    - $\hat {F_C}=\mathrm {Conv}_{1 \times 1}(F_C)$
- 3x3 DWConvë¥¼ 3x3ê³¼ 5x5 DWConvë¡œ channel-wiseë¡œ ë‚˜ëˆ„ì–´ ì—°ì‚° í›„ Concat (Mix-CFN)

    - <img src="https://user-images.githubusercontent.com/103131249/217580303-84047ad0-3197-419f-b83e-ffccf3ca53b9.png" width="180">

    - $\hat {F_C}=\mathrm {Conv}_{1 \times 1}(\mathrm {Concat}(\hat {F_1},\hat {F_2}))$
- Batch-Normalization ì¶”ê°€


### 5. Decode Head

<img src="https://user-images.githubusercontent.com/25689849/217508282-bb070e23-280f-4268-a2cc-2d7021c2eab7.svg">

- Stage Features Upsample
    - <img src="https://user-images.githubusercontent.com/103131249/217580836-fd09f784-b0b8-497b-b58f-7b64466106fd.png" width="200">
    
- **Weighted Sum ì ìš©**
    - <img src="https://user-images.githubusercontent.com/103131249/217582409-9b7e3443-4b72-42c2-8017-2b539fbc8167.png" width="150">


---

## ğŸ“° **Result**

![result graph](https://user-images.githubusercontent.com/25689849/217506475-2bd67040-3f76-4d52-b61d-024bf880c99f.svg)

| model  | Params| Flops | Acc<sub>val<sup> (%) | mIoU<sub>val<sup> (%) |
| :-------------: | :--------------------: | :-----------------------: | :----------------------: | :-----------------------: |
|SegFormer-B2|27.462M|58.576G|66.48|29.84|
|**BoostFormer</br>(Ours)**|**17.575M</br>(-36.00%)**|**15.826G</br>(-72.98%)**|**72.28</br>(+8.72%)**|**34.29</br>(+14.91%)**|
- **ê¸°ì¡´ ëª¨ë¸ ëŒ€ë¹„ Params 36% ê°ì†Œ, FLOPs 72% ê°ì†Œ, mIoU ì„±ëŠ¥ 14% í–¥ìƒ**
<br/><br/>

---
## ğŸ“° **Qualitative results on ADE20K**
![results](https://user-images.githubusercontent.com/103131249/217559500-9d9765f5-96f5-4e2d-81c5-3dd2a89fd7a4.png)

## ğŸ“° **Mobile Inference Time Comparison**
![image](https://user-images.githubusercontent.com/103131249/217567804-d8c6cde7-f991-4039-a409-2783600f0a33.png)



## ğŸ“° **NVIDIA Jetson Nano Time Comparision**
![image](https://user-images.githubusercontent.com/103131249/217698136-70bf4eaf-2709-4689-a0ac-9f6642b83868.png)

---


## âš™ï¸ **Installation**

```bash
git clone https://github.com/boostcampaitech4lv23cv3/final-project-level3-cv-16.git
```

## ğŸ§° **How to Use**
### Pretraining (tiny_imagenet)
```bash
bash dist_train.sh {ì‚¬ìš©í•˜ëŠ” gpu ê°œìˆ˜} \
    --data-path {tiny_imagenet path} \ # ì´ë¦„ì— tinyê°€ í¬í•¨ë˜ì–´ì•¼í•¨
    --output_dir {save dir path} \
    --batch-size {batch size per gpu } # default=128

# example
bash dist_train.sh 4 \
    --data-path /workspace/dataset/tiny_imagenet \
    --output_dir result/mod_segformer/ \
    --batch-size 64

```
### ADE20K fine-tuning
```bash
# í˜„ì¬ ë””ë ‰í† ë¦¬: /final-project-level3-cv-16
python train.py \
    --data_dir {ADE20Kì˜ path} \
    --device 0,1,2,3 \ # í™˜ê²½ì— ë§ê²Œ ìˆ˜ì • 
    --save_path {saveí•˜ê³ ì í•˜ëŠ” dirì˜ path} \ 
    --pretrain {pretrain ëª¨ë¸ dir í˜¹ì€ .pthì˜ path} # .pth(pretrainì˜ output), dir(huggingfaceì˜ ëª¨ë¸í—ˆë¸Œì—ì„œ ì œê³µí•˜ëŠ” í˜•íƒœ)
    --batch_size {batch size} # default=16
```

### Evaluation ìˆ˜í–‰
```bash
# phaseë¥¼ í†µí•´ val ë˜ëŠ” test set ì„¤ì •
python eval.py \ # eval.py ë‚´ì˜ modelì„ ì •ì˜í•˜ëŠ” ì½”ë“œ ìˆ˜ì •
    --data_dir {ADE20Kì˜ path} \
    --pretrain {pretrain ëª¨ë¸ dirì˜ path}
```
### Params, FLOPs í™•ì¸
```bash
python util/get_flops_params.py \ # get_flops_params.py ë‚´ì˜ modelì„ ì •ì˜í•˜ëŠ” ì½”ë“œ ìˆ˜ì •
    --data_dir {ADE20Kì˜ path}
```

---
## ğŸ“° **Directory Structure**

```
|-- ğŸ—‚ appendix          : ë°œí‘œìë£Œ ë° WrapUpReport
|-- ğŸ—‚ segformer         : HuggingFace ê¸°ë°˜ segformer ëª¨ë¸ ì½”ë“œ
|-- ğŸ—‚ boostformer       : Segformer ê²½ëŸ‰í™” ëª¨ë¸ ì½”ë“œ
|-- ğŸ—‚ imagenet_pretrain : Tiny-ImageNet encoder í•™ìŠµì‹œ ì‚¬ìš©í•œ ì½”ë“œ
|-- ğŸ—‚ util              : tools ì½”ë“œ ëª¨ìŒ
|-- Dockerfile
|-- train.py             : ADE20K Finetuning ì½”ë“œ
|-- eval.py              : ëª¨ë¸ Inference ê²°ê³¼ ì¶œë ¥ ì½”ë“œ
|-- requirements.txt
`-- README.md
```
